{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd0c7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=vo6gQz5lYRI&list=PLKnIA16_RmvZo7fp5kkIth6nRTeQQsjfX&index=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f632156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA corpus is a collection of authentic text or audio organized into datasets.\\nThe set of unique words used in the text corpus is referred to as the vocabulary.\\nA document is a distinct text/entry.\\nIndividual word in document is a word in NLP.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A corpus is a collection of authentic text or audio organized into datasets.\n",
    "The set of unique words used in the text corpus is referred to as the vocabulary.\n",
    "A document is a distinct text/entry.\n",
    "Individual word in document is a word in NLP.\n",
    "\n",
    "d1 = 'people watch youtube'\n",
    "d2 = 'youtube watch youtube'\n",
    "d3 = 'people write comment'\n",
    "d4 = 'youtube write comment'\n",
    "\n",
    "corpus = 'people watch youtube youtube watch youtube people write comment youtube write comment'\n",
    "\n",
    "vocabulary = ['people','watch','youtube','write','comment'] here v = 5\n",
    "\n",
    "document = d1, d2, d3, d4\n",
    "\n",
    "words in d1 = ['people', 'watch', 'youtube']\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2e86d",
   "metadata": {},
   "source": [
    "One Hot Encodeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "d1 = 'people watch youtube'\n",
    "d2 = 'youtube watch youtube'\n",
    "d3 = 'people write comment'\n",
    "d4 = 'youtube write comment'\n",
    "\n",
    "d1 = [ [1,0,0,0,0],\n",
    "       [0,1,0,0,0],\n",
    "       [0,0,1,0,0] ]\n",
    "       \n",
    "\n",
    "d2 = [ [0,0,1,0,0],\n",
    "       [0,1,0,0,0],\n",
    "       [0,0,1,0,0] ]\n",
    "    \n",
    "\n",
    "d3 = [ [1,0,0,0,0],\n",
    "       [0,0,0,1,0],\n",
    "       [0,0,0,0,1] ]\n",
    "       \n",
    "\n",
    "d4 = [ [0,0,1,0,0],\n",
    "       [0,0,0,1,0],\n",
    "       [0,0,0,0,1] ]\n",
    "\n",
    "order = (3,5)\n",
    "\n",
    "This method is not used because of Sparcity of data, No fixed size of order matrix, Out of vocabulary, \n",
    "no capturing of semantics \n",
    "\n",
    "'''   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad801b0",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09796ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    " '''\n",
    " Especially used for text classification problem. \n",
    "  \n",
    "                                          Classified As\n",
    "                                           \n",
    "d1 = 'people watch youtube'  ---------->      1\n",
    "d2 = 'youtube watch youtube' ---------->      1\n",
    "d3 = 'people write comment'  ---------->      0\n",
    "d4 = 'youtube write comment' ---------->      0\n",
    "\n",
    "vocabulary = ['people','watch','youtube','write','comment'] here v = 5\n",
    "\n",
    "        | 'people' | 'watch' | 'youtube' | 'write' | 'comment' |\n",
    "\n",
    "    d1       1         1          1            0         0\n",
    "    \n",
    "    d2       0         1          2            0         0\n",
    "    \n",
    "    d3       1         0          0            1         1\n",
    "    \n",
    "    d4       0         0          1            1         1\n",
    "\n",
    "Bag of word assumes that if d1 = d2 = 1 & d3 = d4 = 0 then they must have high similarity \n",
    "and if (d1 = d2 = 1) â‰  (d3 = d4 = 0) meaning they must have less similarity.\n",
    "\n",
    "Measuring the similarity between vectors we can find similarity \n",
    "\n",
    "'''\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a5efaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube watch youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube write comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text\n",
       "0   people watch youtube\n",
       "1  youtube watch youtube\n",
       "2   people write comment\n",
       "3  youtube write comment"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':['people watch youtube', \n",
    "                          'youtube watch youtube',\n",
    "                          'people write comment',\n",
    "                          'youtube write comment']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ff836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163e6828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = cv.fit_transform(df['text'])\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc0a99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 1, 'watch': 2, 'youtube': 4, 'write': 3, 'comment': 0}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0012e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdebeaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "936799da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1]]\n",
      "people watch youtube\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(df['text'][0])#'people watch youtube'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14f7b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 2]]\n",
      "youtube watch youtube\n"
     ]
    }
   ],
   "source": [
    "print(bow[1].toarray())\n",
    "print(df['text'][1])#'youtube watch youtube'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9783ead3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0]]\n",
      "people write comment\n"
     ]
    }
   ],
   "source": [
    "print(bow[2].toarray())\n",
    "print(df['text'][2])#'people write comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9f1b96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 1]]\n",
      "youtube write comment\n"
     ]
    }
   ],
   "source": [
    "print(bow[3].toarray())\n",
    "print(df['text'][3])#'youtube write comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe1923d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 1, 'watch': 2, 'youtube': 4, 'write': 3, 'comment': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 2]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cv.vocabulary_)\n",
    "cv.transform(['youtube watch and write comment of youtube']).toarray()#transforming new sentance to BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0abb74e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nout of context word like 'and', 'of' are handled here\\n\\nIf binary = True\\n\\n        | 'people' | 'watch' | 'youtube' | 'write' | 'comment' |\\n\\n    d1       1         1          1            0         0\\n    \\n    d2       0         1          1            0         0\\n    \\n    d3       1         0          0            1         1\\n    \\n    d4       0         0          1            1         1\\n\\nIt look whether the word is present their or not.\\n\\nProblem of BOW is Sparcity, Out of vocabulary, Ordering, \\n                  sentences like 'This is good movie' and 'This is not good movie' have high similarity despite their\\n                  meaning is different\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "out of context word like 'and', 'of' are handled here\n",
    "\n",
    "If binary = True\n",
    "\n",
    "        | 'people' | 'watch' | 'youtube' | 'write' | 'comment' |\n",
    "\n",
    "    d1       1         1          1            0         0\n",
    "    \n",
    "    d2       0         1          1            0         0\n",
    "    \n",
    "    d3       1         0          0            1         1\n",
    "    \n",
    "    d4       0         0          1            1         1\n",
    "\n",
    "It look whether the word is present their or not.\n",
    "\n",
    "Problem of BOW is Sparcity, Out of vocabulary, Ordering, \n",
    "                  sentences like 'This is good movie' and 'This is not good movie' have high similarity despite their\n",
    "                  meaning is different\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f81db9",
   "metadata": {},
   "source": [
    "N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d9f65c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nN-Grams solves word ordering problem. \\n\\nCreating bi gram OR 2-gram\\n\\nd1 = 'people watch youtube'\\nd2 = 'youtube watch youtube'\\nd3 = 'people write comment'\\nd4 = 'youtube write comment'\\n\\n\\n    | people watch | watch youtube | youtube watch | people write | write comment | youtube write |\\n    \\nd1         1             1               0                 0             0                  0\\n\\nd2         0             1               1                 0             0                  0\\n\\nd3         0             1               0                 1             1                  0 \\n\\nd4         0             0               0                 0             1                  1\\n\\n\\nCreating tri gram OR 3-gram\\n\\n\\n        | people watch youtube | youtube watch youtube | people write comment | youtube write comment |\\n\\nd1                   1                     0                      0                        0\\n\\nd2                   0                     1                      0                        0\\n\\nd3                   0                     0                      1                        0\\n\\nd4                   0                     0                      0                        1\\n \\n\\n\\nCreating quad gram OR 4-gram error will occur here but in real textual data it will be possible because of longer length\\n\\nIf n = 1, uni-gram it becomes Bag Of Words (above method)\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "N-Grams solves word ordering problem. \n",
    "\n",
    "Creating bi gram OR 2-gram\n",
    "\n",
    "d1 = 'people watch youtube'\n",
    "d2 = 'youtube watch youtube'\n",
    "d3 = 'people write comment'\n",
    "d4 = 'youtube write comment'\n",
    "\n",
    "\n",
    "    | people watch | watch youtube | youtube watch | people write | write comment | youtube write |\n",
    "    \n",
    "d1         1             1               0                 0             0                  0\n",
    "\n",
    "d2         0             1               1                 0             0                  0\n",
    "\n",
    "d3         0             1               0                 1             1                  0 \n",
    "\n",
    "d4         0             0               0                 0             1                  1\n",
    "\n",
    "\n",
    "Creating tri gram OR 3-gram\n",
    "\n",
    "\n",
    "        | people watch youtube | youtube watch youtube | people write comment | youtube write comment |\n",
    "\n",
    "d1                   1                     0                      0                        0\n",
    "\n",
    "d2                   0                     1                      0                        0\n",
    "\n",
    "d3                   0                     0                      1                        0\n",
    "\n",
    "d4                   0                     0                      0                        1\n",
    " \n",
    "\n",
    "\n",
    "Creating quad gram OR 4-gram error will occur here but in real textual data it will be possible because of longer length\n",
    "\n",
    "If n = 1, uni-gram it becomes Bag Of Words (above method)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ba1ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer(ngram_range=(2,2))#ngram_range=(min_n,max_n) bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b451327b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = cv.fit_transform(df['text'])\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "701ec8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people watch': 0, 'watch youtube': 2, 'youtube watch': 4, 'people write': 1, 'write comment': 3, 'youtube write': 5}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)#bi-gram case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21e2f7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae551138",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(ngram_range=(1,2))#ngram_range=(min_n,max_n) uni-gram + bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3710ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = cv.fit_transform(df['text'])\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd9c765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 1, 'watch': 4, 'youtube': 8, 'people watch': 2, 'watch youtube': 5, 'youtube watch': 9, 'write': 6, 'comment': 0, 'people write': 3, 'write comment': 7, 'youtube write': 10}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41de260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n b1 = 'this movie is very good'\\n b2 = 'this movie is not very good'\\n \\n Example of bigram being superior to BOW \\n \\n BOW\\n \\n        | this | movie | is | very | good | not |\\n \\nb1         1       1     1     1      1      0\\n \\nb2         1       1     1     1      1      1\\n \\n \\ncosinesimilarity =  (1*1 + 1*1 + 1*1 + 1*1 + 1*1 + 0*1)/(sqrt(1^2+1^2+1^2+1^2+1^2)*sqrt(1^2+1^2+1^2+1^2+1^2+1^2)) \\n                 = 5/sqrt(30) ~ 24Â°\\n \\n==================================================================================================================== \\n \\n        | this movie | movie is | is very | very good | is not | not very |\\n        \\nb1            1           1          1         1          0         0\\n\\nb2            1           1          0         1          1         1\\n \\ncosine similarity = (1*1 + 1*1 + 1*0 + 1*1 + 0*1 + 0*1)/(sqrt(1^2+1^2+1^2+1^2)*sqrt(1^2+1^2+1^2+1^2+1^2)) = 3/sqrt(20) = 0.67\\n\\nthitha = acos(0.67) ~ 48Â°\\n\\nLess angle = More similarity\\n\\nIt captures semantic meaning\\nBut number of tokens for large data set increases as value of N increases. Also, has no solution for out of \\nvocabulary terms.\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " b1 = 'this movie is very good'\n",
    " b2 = 'this movie is not very good'\n",
    " \n",
    " Example of bigram being superior to BOW \n",
    " \n",
    " BOW\n",
    " \n",
    "        | this | movie | is | very | good | not |\n",
    " \n",
    "b1         1       1     1     1      1      0\n",
    " \n",
    "b2         1       1     1     1      1      1\n",
    " \n",
    " \n",
    "cosinesimilarity =  (1*1 + 1*1 + 1*1 + 1*1 + 1*1 + 0*1)/(sqrt(1^2+1^2+1^2+1^2+1^2)*sqrt(1^2+1^2+1^2+1^2+1^2+1^2)) \n",
    "                 = 5/sqrt(30) ~ 24Â°\n",
    " \n",
    "==================================================================================================================== \n",
    " \n",
    "        | this movie | movie is | is very | very good | is not | not very |\n",
    "        \n",
    "b1            1           1          1         1          0         0\n",
    "\n",
    "b2            1           1          0         1          1         1\n",
    " \n",
    "cosine similarity = (1*1 + 1*1 + 1*0 + 1*1 + 0*1 + 0*1)/(sqrt(1^2+1^2+1^2+1^2)*sqrt(1^2+1^2+1^2+1^2+1^2)) = 3/sqrt(20) = 0.67\n",
    "\n",
    "thitha = acos(0.67) ~ 48Â°\n",
    "\n",
    "Less angle = More similarity\n",
    "\n",
    "It captures semantic meaning\n",
    "But number of tokens for large data set increases as value of N increases. Also, has no solution for out of \n",
    "vocabulary terms.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21c353",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd17b491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe problem with BOW, N-Gram is that it gives equal importance to every words  \\n\\nd1 = 'people watch youtube'\\nd2 = 'youtube watch youtube'\\nd3 = 'people write comment'\\nd4 = 'youtube write comment'\\n\\nvocabulary = ['people','watch','youtube','write','comment'] here v = 5\\n\\n        | 'people' | 'watch' | 'youtube' | 'write' | 'comment' |\\n\\n    d1       1         1          1            0         0\\n    \\n    d2       0         1          2            0         0\\n    \\n    d3       1         0          0            1         1\\n    \\n    d4       0         0          1            1         1\\n    \\nHere, BOW/1-Gram gave equal importance to 'people' and 'watch' but TF-IDF gives more importance to a word if its\\nrepetation in other document is low\\n\\n\\nd1 = 'people watch youtube'\\nd2 = 'youtube watch youtube'\\nd3 = 'people write comment'\\nd4 = 'youtube write comment'\\n    \\nHere some words will have higher importance as and other will have less importance   \\n    \\nTo calculate importance we will find TF = Term Frequency              &\\n                                    IDF = Inverse Document Frequency \\n    \\nWord will have higher weightage if its occurance in document is more and less in other document.\\n\\nTF (t, d) = Number_of_occurance_of_term_t_in_document_d/Total_number_of_terms_in_document_d    \\n    \\nIDF (t) = log_e ( Total_number_of_documents_in_the_corpus / Number_of_documents_with_term_t_in_them  )\\n\\nvocabulary = ['people','watch','play','youtube','write','comment'] here v = 6\\n\\n\\nIF(people,d1)1/3 = d1_has_1_people / d1_has_3_words\\n\\n               IF_d1            IF_d2           IF_d3           IF_d4      \\n\\npeople    IF(people,d1)1/3      0               1/3             0\\nwatch          1/3              1/3             0               0\\nyoutube        1/3              2/3             0               1/3\\nwrite          0                0               1/3             1/3\\ncomment        0                0               1/3             1/3\\n\\n\\nd1 = 'people watch youtube'\\nd2 = 'youtube watch youtube'\\nd3 = 'people write comment'\\nd4 = 'youtube write comment'\\n\\nIDF_d1(youtube) = log( 4 / 3 ) = log( their_are_4_documents_in_corpus / their_are_3_document_with_'youtube' )\\n\\n               IDF             \\n\\npeople         log(4/2) ~ 0.301\\nwatch          log(4/2) ~ 0.301\\nyoutube        log(4/3) ~ 0.125\\nwrite          log(4/2) ~ 0.301\\ncomment        log(4/2) ~ 0.301\\n\\nIDF (t) = log_e ( Total_number_of_documents_in_the_corpus / Number_of_documents_with_term_t_in_them  )\\n\\nIDF measures rarity of a word here \\n                                        Total_number_of_documents_in_the_corpus is always fixed\\n                                        \\n                                        Number_of_documents_with_term_t_in_them gives rarity \\n                                                  |  \\n                                                   -> if a term occurs is every document then its not rare\\n                                                   -> if a term occurs is few document then its rare\\n                                                   \\n\\nIF Total_number_of_documents_in_the_corpus =  Number_of_documents_with_term_t_in_them then log() = 1\\n\\nIF Total_number_of_documents_in_the_corpus >>  Number_of_documents_with_term_t_in_them then log() >> 1\\n\\nSo, TF-IDF = TF * IDF\\n\\nTF-TDF for above text is\\n\\n               IF_IDF         IF_IDF          IF_IDF         IF_IDF      \\n\\npeople         1/3*0.301        0               1/3*0.301       0\\nwatch          1/3*0.301        1/3*0.301       0               0\\nyoutube        1/3*0.125        2/3*0.125       0               1/3*0.125\\nwrite          0                0               1/3*0.301       1/3*0.301\\ncomment        0                0               1/3*0.301       1/3*0.301\\n\\n=====================================================================================================================\\n\\n               IF_IDF         IF_IDF          IF_IDF         IF_IDF      \\n\\npeople         0.1              0               0.1             0\\nwatch          0.1              0.1             0               0\\nyoutube        0.04             0.08            0               0.04\\nwrite          0                0               0.1             0.1\\ncomment        0                0               0.1             0.1\\n\\n=====================================================================================================================\\n\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The problem with BOW, N-Gram is that it gives equal importance to every words  \n",
    "\n",
    "d1 = 'people watch youtube'\n",
    "d2 = 'youtube watch youtube'\n",
    "d3 = 'people write comment'\n",
    "d4 = 'youtube write comment'\n",
    "\n",
    "vocabulary = ['people','watch','youtube','write','comment'] here v = 5\n",
    "\n",
    "        | 'people' | 'watch' | 'youtube' | 'write' | 'comment' |\n",
    "\n",
    "    d1       1         1          1            0         0\n",
    "    \n",
    "    d2       0         1          2            0         0\n",
    "    \n",
    "    d3       1         0          0            1         1\n",
    "    \n",
    "    d4       0         0          1            1         1\n",
    "    \n",
    "Here, BOW/1-Gram gave equal importance to 'people' and 'watch' but TF-IDF gives more importance to a word if its\n",
    "repetation in other document is low\n",
    "\n",
    "\n",
    "d1 = 'people watch youtube'\n",
    "d2 = 'youtube watch youtube'\n",
    "d3 = 'people write comment'\n",
    "d4 = 'youtube write comment'\n",
    "    \n",
    "Here some words will have higher importance as and other will have less importance   \n",
    "    \n",
    "To calculate importance we will find TF = Term Frequency              &\n",
    "                                    IDF = Inverse Document Frequency \n",
    "    \n",
    "Word will have higher weightage if its occurance in document is more and less in other document.\n",
    "\n",
    "TF (t, d) = Number_of_occurance_of_term_t_in_document_d/Total_number_of_terms_in_document_d    \n",
    "    \n",
    "IDF (t) = log_e ( Total_number_of_documents_in_the_corpus / Number_of_documents_with_term_t_in_them  )\n",
    "\n",
    "vocabulary = ['people','watch','play','youtube','write','comment'] here v = 6\n",
    "\n",
    "\n",
    "IF(people,d1)1/3 = d1_has_1_people / d1_has_3_words\n",
    "\n",
    "               IF_d1            IF_d2           IF_d3           IF_d4      \n",
    "\n",
    "people    IF(people,d1)1/3      0               1/3             0\n",
    "watch          1/3              1/3             0               0\n",
    "youtube        1/3              2/3             0               1/3\n",
    "write          0                0               1/3             1/3\n",
    "comment        0                0               1/3             1/3\n",
    "\n",
    "\n",
    "d1 = 'people watch youtube'\n",
    "d2 = 'youtube watch youtube'\n",
    "d3 = 'people write comment'\n",
    "d4 = 'youtube write comment'\n",
    "\n",
    "IDF_d1(youtube) = log( 4 / 3 ) = log( their_are_4_documents_in_corpus / their_are_3_document_with_'youtube' )\n",
    "\n",
    "               IDF             \n",
    "\n",
    "people         log(4/2) ~ 0.301\n",
    "watch          log(4/2) ~ 0.301\n",
    "youtube        log(4/3) ~ 0.125\n",
    "write          log(4/2) ~ 0.301\n",
    "comment        log(4/2) ~ 0.301\n",
    "\n",
    "IDF (t) = log_e ( Total_number_of_documents_in_the_corpus / Number_of_documents_with_term_t_in_them  )\n",
    "\n",
    "IDF measures rarity of a word here \n",
    "                                        Total_number_of_documents_in_the_corpus is always fixed\n",
    "                                        \n",
    "                                        Number_of_documents_with_term_t_in_them gives rarity \n",
    "                                                  |  \n",
    "                                                   -> if a term occurs is every document then its not rare\n",
    "                                                   -> if a term occurs is few document then its rare\n",
    "                                                   \n",
    "\n",
    "IF Total_number_of_documents_in_the_corpus =  Number_of_documents_with_term_t_in_them then log() = 1\n",
    "\n",
    "IF Total_number_of_documents_in_the_corpus >>  Number_of_documents_with_term_t_in_them then log() >> 1\n",
    "\n",
    "So, TF-IDF = TF * IDF\n",
    "\n",
    "TF-TDF for above text is\n",
    "\n",
    "               IF_IDF         IF_IDF          IF_IDF         IF_IDF      \n",
    "\n",
    "people         1/3*0.301        0               1/3*0.301       0\n",
    "watch          1/3*0.301        1/3*0.301       0               0\n",
    "youtube        1/3*0.125        2/3*0.125       0               1/3*0.125\n",
    "write          0                0               1/3*0.301       1/3*0.301\n",
    "comment        0                0               1/3*0.301       1/3*0.301\n",
    "\n",
    "=====================================================================================================================\n",
    "\n",
    "               IF_IDF         IF_IDF          IF_IDF         IF_IDF      \n",
    "\n",
    "people         0.1              0               0.1             0\n",
    "watch          0.1              0.1             0               0\n",
    "youtube        0.04             0.08            0               0.04\n",
    "write          0                0               0.1             0.1\n",
    "comment        0                0               0.1             0.1\n",
    "\n",
    "=====================================================================================================================\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6568db35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube watch youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youtube write comment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text\n",
       "0   people watch youtube\n",
       "1  youtube watch youtube\n",
       "2   people write comment\n",
       "3  youtube write comment"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d9bfd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.61366674, 0.61366674, 0.        , 0.49681612],\n",
       "       [0.        , 0.        , 0.52546357, 0.        , 0.8508161 ],\n",
       "       [0.57735027, 0.57735027, 0.        , 0.57735027, 0.        ],\n",
       "       [0.61366674, 0.        , 0.        , 0.61366674, 0.49681612]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(df['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d2a2ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 1, 'watch': 2, 'youtube': 4, 'write': 3, 'comment': 0}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec939ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.51082562 1.51082562 1.51082562 1.51082562 1.22314355]\n",
      "['comment' 'people' 'watch' 'write' 'youtube']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idf_)\n",
    "print(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0634785e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNotice that our calculated value is different from above becauyse above algorithm uses\\n\\n   IDF(t) = log ( n / df(t)) + 1\\n    \\n   1 is added because for word that repeats in every term will not be ignored\\n   \\n   Ideal textbook defines formula as \\n   \\n   DF(t) = log ( n / (df(t)+1) )\\n   \\n   \\nAnother Qn can be why take log  in IDF?\\n\\nIf IDF = N/n\\n\\nSuppose 10,000 movie review we have a common word 'the' then IDF = 1 = 10000/10000\\n\\n                            we have rare word 'encyclopedia' then IDF = 10000 = 10000/1\\n                            \\nwhich will be a problem because 10,000 is very large factor as it will dominate TF * IDF product\\n\\nif log is used 'the' IDF = 0\\nif log is used 'encyclopedia' IDF = 4\\n\\nTF-IDF is used in information retrieval system.\\n\\nIf vocabulary = v is big then then we might bet lot of sparse array\\nOut of vocabulary problem\\nDimensionality increases as v increases and brings problem of overfitting\\nCant capture semantic relation\\n          This is beautiful\\n          This is gorgeous\\n \\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Notice that our calculated value is different from above becauyse above algorithm uses\n",
    "\n",
    "   IDF(t) = log ( n / df(t)) + 1\n",
    "    \n",
    "   1 is added because for word that repeats in every term will not be ignored\n",
    "   \n",
    "   Ideal textbook defines formula as \n",
    "   \n",
    "   DF(t) = log ( n / (df(t)+1) )\n",
    "   \n",
    "   \n",
    "Another Qn can be why take log  in IDF?\n",
    "\n",
    "If IDF = N/n\n",
    "\n",
    "Suppose 10,000 movie review we have a common word 'the' then IDF = 1 = 10000/10000\n",
    "\n",
    "                            we have rare word 'encyclopedia' then IDF = 10000 = 10000/1\n",
    "                            \n",
    "which will be a problem because 10,000 is very large factor as it will dominate TF * IDF product\n",
    "\n",
    "if log is used 'the' IDF = 0\n",
    "if log is used 'encyclopedia' IDF = 4\n",
    "\n",
    "TF-IDF is used in information retrieval system.\n",
    "\n",
    "If vocabulary = v is big then then we might bet lot of sparse array\n",
    "Out of vocabulary problem\n",
    "Dimensionality increases as v increases and brings problem of overfitting\n",
    "Cant capture semantic relation\n",
    "          This is beautiful\n",
    "          This is gorgeous\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf5fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
